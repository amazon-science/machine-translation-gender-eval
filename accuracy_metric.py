"""
Automatic accuracy metric for MT-GenEval paper.
We use the correct (gender-matched) references and incorrect (counterfactual/swapped) references to evaluate accuracy.
We first check the references for unique/gender-specific terms, then check for overlap with the incorrect gender
and the hypothesis. If there is overlap, we mark the segment as incorrect.
"""
import os
import string
import argparse


STRIP_PUNCT = str.maketrans(string.punctuation, ' '*len(string.punctuation))


def clean_line(line):
    """
    For an input line, lowercase it, strip extra spaces, and replace punctuation with spaces.

    :param line: Line to clean.
    :return: Cleaned version of the line
    """
    return line.lower().translate(STRIP_PUNCT).strip()


def get_words(line):
    """
    Helper function to get the set of words in a line.

    :param line: Line from which to get the words.
    :return: Set of words in the line.
    """
    return set(line.strip().split())


def get_trg_correct_incorrect(trg_line, orig_ref, ctf_ref):
    """
    Compute overlap between references and translation
    We first get unique words in each of the references w.r.t each other then we compute their overlap with target
    """ 
    # get words for each segment
    trg_words, orig_words, ctf_words = get_words(trg_line), get_words(orig_ref), get_words(ctf_ref)
    # get unique words in each of the references
    orig_unique = orig_words - ctf_words
    ctf_unique = ctf_words - orig_words
    # now check the words in the target sentence for overlap with incorrect unique words
    trg_correct = trg_words & orig_unique 
    trg_incorrect = trg_words & ctf_unique
    return trg_correct, trg_incorrect 


def gender_decision(trg_line, orig_ref, ctf_ref):
    """
    Check if gender of a sentence is correct based on corresponding correct and incorrect references.
    Algorithm: We make decision based on whether hyp overlaps with original ref and counterfactual ref

    :param trg_line: Sentence from translation output for which to check gender.
    :param orig_ref: Original (Correct) reference.
    :param ctf_ref: Counterfactual reference.
    :return: a list of decision, overlap(hyp, original ref), overlap(hyp, counterfactual ref)
    """
    trg_correct, trg_incorrect = get_trg_correct_incorrect(trg_line, orig_ref, ctf_ref)

    if trg_incorrect:
        decision = 'Incorrect'
    else:
        decision = 'Correct'

    return [decision, trg_correct, trg_incorrect]


def read_file_to_list(filename):
    """
    Read a file to a list of lines; each line will be lowercased and punctuation will be removed.

    :param filename: File to read.
    :return: List of lines of the file.
    """
    out_list = []
    with open(filename, 'r') as infile:
        for line in infile:
            cleaned_line = clean_line(line)
            out_list.append(cleaned_line)
    return out_list


def accuracy_metric(hypothesis, orig_ref, ctf_ref):
    """
    Compute whether gender is correct in the translation given actual reference and counterfactual reference
    We assume input files contain one sentence per line
    :param hypothesis: AMT hypothesis file to be evaluated
    :param orig_ref: Correctly gendered reference file path
    :param ctf_ref: Incorrect (counterfactual) reference file path
    :return decision (Corrct/Incorrect), one per each line in the input files
    """
    # read in the files. 
    # Note that each line of the input files will be lowercased and punctuation will be removed. 
    trg_list = read_file_to_list(hypothesis)
    cor_list = read_file_to_list(orig_ref)
    inc_list = read_file_to_list(ctf_ref)

    assert len(trg_list) == len(cor_list), f'Output file and original reference file must have the same number of lines. Files are {hypothesis}, {orig_ref}'
    assert len(trg_list) == len(inc_list), f'Output file and counterfactual reference file must have the same number of lines. Files are {hypothesis},  {ctf_ref}'

    metric_annot_mapped = []    
    for trg_line, orig_ref, ctf_ref in zip(trg_list, cor_list, inc_list):
        [decision, trg_correct, trg_incorrect] = gender_decision(trg_line, orig_ref, ctf_ref)
        metric_annot_mapped.append(decision) 
    accuracy = metric_annot_mapped.count('Correct')/len(metric_annot_mapped)
    return accuracy, metric_annot_mapped


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--target_lang', type=str, choices=['ar', 'de', 'es', 'fr', 'hi', 'it', 'pt', 'ru'], 
                        help='Target language codes. Arabic->ar, German->de, Spanish->es, \
                        French->fr, Hindi->hi, Italian->it, Portuguese->pt, Russian->ru')
    parser.add_argument('--dataset', type=str, choices=['contextual', 'counterfactual'], help='Type of MTGenEval dataset to be evaluated')
    parser.add_argument('--data_split', type=str, choices=['dev', 'test'], help='MTGenEval data split to be evaluated')
    parser.add_argument('--hyp', type=str, help='System translations path for contextual dataset', default=None)
    parser.add_argument('--hyp_masculine', type=str, 
                        help='System translations path for masculine segments in the case of counterfactual subset', default=None)
    parser.add_argument('--hyp_feminine', type=str, 
                        help='System translations path for feminine segments in the case of counterfactual subset', default=None)
    args = parser.parse_args()
    return args


def main():
    """
    Computes accuracy for the counterfactual and contextual datasets
    """
    args = parse_args()

    if args.data_split not in ['dev', 'test']:
        raise ValueError(f'Invalid argument for data_split {args.data_split}. Valid options are dev and test')

    current_dir = os.path.dirname(os.path.abspath(__file__))
    if args.dataset == 'contextual':
        ref_path = f'{current_dir}/data/context/geneval-context-wikiprofessions-original-{args.data_split}.en_{args.target_lang}.{args.target_lang}'
        flipped_ref_path = f'{current_dir}/data/context/geneval-context-wikiprofessions-flipped-{args.data_split}.en_{args.target_lang}.{args.target_lang}'
        accuracy, metric_decisions = accuracy_metric(args.hyp, ref_path, flipped_ref_path)
    elif args.dataset == 'counterfactual':
        masculine_ref_path = f'{current_dir}/data/sentences/{args.data_split}/geneval-sentences-masculine-{args.data_split}.en_{args.target_lang}.{args.target_lang}'
        feminine_ref_path = f'{current_dir}/data/sentences/{args.data_split}/geneval-sentences-feminine-{args.data_split}.en_{args.target_lang}.{args.target_lang}'
        _, metric_decisions_masculine = accuracy_metric(args.hyp_masculine, masculine_ref_path, feminine_ref_path)
        _, metric_decisions_feminine = accuracy_metric(args.hyp_feminine, feminine_ref_path, masculine_ref_path)
        # combined_decision -- 'Correct' only if both metric_decisions_masculine/feminine are 'Correct'
        combined_decision = ['Incorrect' if 'Incorrect' in [d1,d2] else 'Correct' for d1,d2 in zip(metric_decisions_masculine, metric_decisions_feminine)]
        accuracy = combined_decision.count('Correct')/len(combined_decision)
    else:
        raise ValueError(f'Invalid argument for dataset {args.dataset}. Valid options are contextual, counterfactual')

    print(f'MTGenEval Accuracy for the en_{args.target_lang} {args.dataset} {args.data_split} subset is {accuracy}')
    

if __name__ == '__main__':
    main()



